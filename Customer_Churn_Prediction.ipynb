{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Data Science",
      "language": "python",
      "name": "ds"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Customer Churn Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VGIxprgxFXGM"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:53.949851Z",
          "start_time": "2020-09-06T19:27:52.149931Z"
        },
        "id": "riD-aZ5LFXDC"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.core.pylabtools import figsize\n",
        "\n",
        "\n",
        "sns.set(font_scale=2)\n",
        "figsize(20, 20)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "pd.set_option('display.max_columns', 50)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:53.965841Z",
          "start_time": "2020-09-06T19:27:53.951849Z"
        },
        "id": "JSBQlpATFXDI"
      },
      "source": [
        "figsize(20, 20)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A1E6lhZFXDO"
      },
      "source": [
        "# Exploratory Data Analysis and Preprocessing\n",
        "\n",
        "**We start by analyzing the data at hand**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:54.155803Z",
          "start_time": "2020-09-06T19:27:53.967839Z"
        },
        "id": "cRTbLFxHFXDP",
        "outputId": "92621428-bd92-483b-ba73-f4ff9c9d0b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "telco_df = pd.read_csv('telco.csv')\n",
        "\n",
        "telco_df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f3509bb1ccc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtelco_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'telco.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtelco_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'telco.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-05T11:17:18.086014Z",
          "start_time": "2020-09-05T11:17:18.080016Z"
        },
        "id": "xmiPfiK1FXDV"
      },
      "source": [
        "**The dataset is really small. This can impose some though choices when it comes to outliers and feature engineering.**\n",
        "\n",
        "**Throwing out outliers is very hard in this case as the information value of each row is high with such low row numbers.**\n",
        "\n",
        "**If we engineer too many features the column/row ration will get very big, resulting in overfitting**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:54.187796Z",
          "start_time": "2020-09-06T19:27:54.157801Z"
        },
        "id": "v9HpO8PqFXDW"
      },
      "source": [
        "telco_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-05T11:10:02.288396Z",
          "start_time": "2020-09-05T11:10:02.269406Z"
        },
        "id": "KiG9h-rTFXDa"
      },
      "source": [
        "## Data Quality checks\n",
        "\n",
        "**Checking the datatypes. It seems retire field should be an int, as it is a flag.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:54.202787Z",
          "start_time": "2020-09-06T19:27:54.189795Z"
        },
        "id": "S1t3VNqIFXDb"
      },
      "source": [
        "telco_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:54.218792Z",
          "start_time": "2020-09-06T19:27:54.204786Z"
        },
        "id": "cYTcJmcdFXDp"
      },
      "source": [
        "telco_df['retire'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:56.912864Z",
          "start_time": "2020-09-06T19:27:56.892842Z"
        },
        "id": "BY-0NhyzFXDw"
      },
      "source": [
        "telco_df['retire'] = telco_df['retire'].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-05T11:12:15.917704Z",
          "start_time": "2020-09-05T11:12:15.901715Z"
        },
        "id": "VdL3EZCkFXD1"
      },
      "source": [
        "**Checking for missing values. Seems all is good. Since there are no string based fields we don't have to check for question marks or empty strings in rows.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:27:58.456197Z",
          "start_time": "2020-09-06T19:27:56.915863Z"
        },
        "id": "vS13p3L4FXD1"
      },
      "source": [
        "sns.heatmap(telco_df.isnull(), cbar = False, cmap = 'viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCfVYGXbFXD4"
      },
      "source": [
        "**Checking correlation matrix to see if we have any redundant columns.**\n",
        "\n",
        "**We don't exactly have columns that are 1:1 correlated to another. However longmon and equipmon are very highly correlated, they are not exactly identical. They might come in use at feature engineering stage so I keep them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:01.784735Z",
          "start_time": "2020-09-06T19:27:58.459194Z"
        },
        "id": "3uEyr8i0FXD5"
      },
      "source": [
        "sns.set(font_scale=1)\n",
        "sns.heatmap(telco_df[telco_df.columns[:21].insert(0,'churn')].corr(), annot=True, cmap='coolwarm', vmin=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:03.617700Z",
          "start_time": "2020-09-06T19:28:01.786750Z"
        },
        "id": "ZIqj9Q_fFXD8"
      },
      "source": [
        "sns.heatmap(telco_df[telco_df.columns[21:]].corr(), annot=True, cmap='coolwarm', vmin=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T00:16:29.069342Z",
          "start_time": "2020-09-06T00:16:29.055351Z"
        },
        "id": "f2EM-yhyFXEA"
      },
      "source": [
        "**As we can see the dataset is imbalanced towards no churn. We will have to keep this in mind to avoid our models overfitting on the imbalance.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:03.633704Z",
          "start_time": "2020-09-06T19:28:03.618700Z"
        },
        "id": "uZgcMwStFXEB"
      },
      "source": [
        "telco_df['churn'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfX01CsPFXEF"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "**Let's see the distributions of the columns to get some ideas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:53.937550Z",
          "start_time": "2020-09-06T19:28:03.635728Z"
        },
        "id": "fkKh29ioFXEF"
      },
      "source": [
        "f, axes_array = plt.subplots(len(telco_df.columns), figsize=(15,15 * len(telco_df.columns)))\n",
        "\n",
        "for i, column in enumerate(telco_df.columns.drop('churn')):\n",
        "    g = sns.FacetGrid(telco_df, hue=\"churn\", )\n",
        "\n",
        "    g = g.map(sns.distplot, column, rug=True, ax=axes_array[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:55.238302Z",
          "start_time": "2020-09-06T19:28:53.942547Z"
        },
        "id": "4Q6epULQFXEJ"
      },
      "source": [
        "telco_df['employ'].value_counts()[0]\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'employ', rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sJSxeKHFXEL"
      },
      "source": [
        "**Does this mean these customers are unemployed, or that they have just switched employer?**\n",
        "\n",
        "**Let's see if we can find different patterns in churn for them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:55.268301Z",
          "start_time": "2020-09-06T19:28:55.240301Z"
        },
        "id": "QPiBMyfyFXEM"
      },
      "source": [
        "def is_not_zero(value):\n",
        "    if value > 0:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "telco_df['is_employed'] = telco_df['employ'].apply(is_not_zero)\n",
        "telco_df[['is_employed', 'churn']].groupby('is_employed').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRq0veF1FXEO"
      },
      "source": [
        "**It seems it is worth keeping this field as this flag (whether our employment assumption is true or not) seems to highly correlate with churn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:56.454338Z",
          "start_time": "2020-09-06T19:28:55.270300Z"
        },
        "id": "9FNWWl8CFXEP"
      },
      "source": [
        "print(telco_df['tollmon'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'tollmon', rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-04T16:36:26.331666Z",
          "start_time": "2020-09-04T16:36:26.324671Z"
        },
        "id": "NQ1uMdhBFXET"
      },
      "source": [
        "**_tollmon_ shows a normal distribution except for the many zeros. Might worth adding a flag for this.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:56.485849Z",
          "start_time": "2020-09-06T19:28:56.456360Z"
        },
        "id": "3SgeKw9-FXET"
      },
      "source": [
        "telco_df['has_tollfree'] = telco_df['tollmon'].apply(is_not_zero)\n",
        "telco_df[['has_tollfree', 'churn']].groupby('has_tollfree').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMNaVj7FFXEX"
      },
      "source": [
        "**It seems this field doesn't mean much of a difference in churn regards, we drop this field**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:56.500839Z",
          "start_time": "2020-09-06T19:28:56.487847Z"
        },
        "id": "kMEFk90oFXEX"
      },
      "source": [
        "telco_df = telco_df.drop('has_tollfree', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcatGJIFXEa"
      },
      "source": [
        "**We will do the same checks for _equipmon_, _cardmon_, _wiremon_, _tollten_, _equipten_, _cardten_, _wireten_**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:57.587785Z",
          "start_time": "2020-09-06T19:28:56.502838Z"
        },
        "id": "yRfzOgpkFXEb"
      },
      "source": [
        "print(telco_df['equipmon'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'equipmon', rug=True)\n",
        "telco_df['has_equipment'] = telco_df['equipmon'].apply(is_not_zero)\n",
        "telco_df[['has_equipment', 'churn']].groupby('has_equipment').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:58.820387Z",
          "start_time": "2020-09-06T19:28:57.589784Z"
        },
        "id": "cwVqfBL2FXEd"
      },
      "source": [
        "print(telco_df['cardmon'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'cardmon', rug=True)\n",
        "telco_df['has_card'] = telco_df['cardmon'].apply(is_not_zero)\n",
        "telco_df[['has_card', 'churn']].groupby('has_card').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:28:59.816085Z",
          "start_time": "2020-09-06T19:28:58.822386Z"
        },
        "id": "zTh-UrMWFXEh"
      },
      "source": [
        "print(telco_df['wiremon'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'wiremon', rug=True)\n",
        "telco_df['has_wireless'] = telco_df['wiremon'].apply(is_not_zero)\n",
        "telco_df[['has_wireless', 'churn']].groupby('has_wireless').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:01.129844Z",
          "start_time": "2020-09-06T19:28:59.818084Z"
        },
        "id": "ybOwgpGcFXEk"
      },
      "source": [
        "print(telco_df['tollten'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'tollten', rug=True)\n",
        "telco_df['has_tollten'] = telco_df['tollten'].apply(is_not_zero)\n",
        "telco_df[['has_tollten', 'churn']].groupby('has_tollten').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:02.299636Z",
          "start_time": "2020-09-06T19:29:01.131843Z"
        },
        "id": "XqMVBt3YFXEm"
      },
      "source": [
        "telco_df = telco_df.drop('has_tollten', axis=1)\n",
        "print(telco_df['equipten'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'equipten', rug=True)\n",
        "telco_df['has_equipten'] = telco_df['equipten'].apply(is_not_zero)\n",
        "telco_df[['has_equipten', 'churn']].groupby('has_equipten').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:03.528261Z",
          "start_time": "2020-09-06T19:29:02.301636Z"
        },
        "id": "bo4H0YJ3FXEp"
      },
      "source": [
        "print(telco_df['cardten'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'cardten', rug=True)\n",
        "telco_df['has_cardten'] = telco_df['cardten'].apply(is_not_zero)\n",
        "telco_df[['has_cardten', 'churn']].groupby('has_cardten').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:04.737319Z",
          "start_time": "2020-09-06T19:29:03.530260Z"
        },
        "id": "mRi26T01FXEs"
      },
      "source": [
        "print(telco_df['wireten'].value_counts()[0])\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'wireten', rug=True)\n",
        "telco_df['has_wireten'] = telco_df['wireten'].apply(is_not_zero)\n",
        "telco_df[['has_wireten', 'churn']].groupby('has_wireten').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-04T17:17:53.107506Z",
          "start_time": "2020-09-04T17:17:53.100510Z"
        },
        "id": "KLO43e6fFXEw"
      },
      "source": [
        "**Let's check if the newly introduced fields are redundant.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:05.923715Z",
          "start_time": "2020-09-06T19:29:04.739318Z"
        },
        "id": "iGLoKkxGFXEx"
      },
      "source": [
        "sns.set(font_scale=1.5)\n",
        "\n",
        "df_new_fields = telco_df[['has_wireten', 'has_cardten', 'has_equipten', 'has_wireless', 'has_card', 'has_equipment', 'is_employed', 'churn']]\n",
        "sns.heatmap(df_new_fields.corr(), annot=True, cmap='coolwarm', vmin=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT3_vOlqFXE0"
      },
      "source": [
        "**It seems the fields we derived from over tenure usage and basic usage contain the same information, so we can drop one of them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:05.938728Z",
          "start_time": "2020-09-06T19:29:05.926733Z"
        },
        "id": "aOPqn-wxFXE1"
      },
      "source": [
        "telco_df = telco_df.drop(['has_wireten', 'has_cardten', 'has_equipten'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmGE_WwLFXE3"
      },
      "source": [
        "### Binning\n",
        "\n",
        "**Sometimes it is worth doing binning of interval data to make it more simple and understandable for man and machine alike**\n",
        "\n",
        "**Looking at the data we see 2 potential columns for binning: tenure and age**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:06.913364Z",
          "start_time": "2020-09-06T19:29:05.940727Z"
        },
        "id": "0ohEWgICFXE4"
      },
      "source": [
        "telco_df['tenure_years'] = telco_df['tenure'] / 12\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'tenure_years', rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:07.880359Z",
          "start_time": "2020-09-06T19:29:06.915362Z"
        },
        "id": "d1GpFlijFXE7"
      },
      "source": [
        "def age_binner(age):\n",
        "    if age < 25:\n",
        "        return 1\n",
        "    if age < 36:\n",
        "        return 2\n",
        "    if age < 45:\n",
        "        return 3\n",
        "    if age < 65:\n",
        "        return 4\n",
        "    return 5\n",
        "\n",
        "telco_df['age_group'] = telco_df['age'].apply(age_binner)\n",
        "g = sns.FacetGrid(telco_df, hue='churn', height=20)\n",
        "g = g.map(sns.distplot, 'age_group', rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:08.505212Z",
          "start_time": "2020-09-06T19:29:07.885356Z"
        },
        "id": "3ry-JY0xFXE-"
      },
      "source": [
        "df_new_fields = telco_df[['tenure', 'tenure_years', 'age', 'age_group', 'churn']]\n",
        "sns.heatmap(df_new_fields.corr(), annot=True, cmap='coolwarm', vmin=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epw9PMGIFXFC"
      },
      "source": [
        "**It seems binning didn't really help in this case, so we drop these columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:08.520202Z",
          "start_time": "2020-09-06T19:29:08.508209Z"
        },
        "id": "isG71GvSFXFC"
      },
      "source": [
        "telco_df = telco_df.drop(['tenure_years', 'age_group'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfmsXSYWFXFF"
      },
      "source": [
        "### One-Hot encoding\n",
        "\n",
        "Numerical nominal fields are to be seperated into indicator fields for each possible values. Leaving a numerical nominal field as it is is wrong, because some models are distance based and assume ordinality between the numbers. \n",
        "\n",
        "In some cases this can work, for example a in field _settlement type_ we could assume ordinality between settlment types going from smaller to larger. \n",
        "\n",
        "In the case of _region_ I assume there is no such ordinality, so I will use One-Hot Encoding for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:08.566174Z",
          "start_time": "2020-09-06T19:29:08.522201Z"
        },
        "id": "N3OMzc6fFXFF"
      },
      "source": [
        "telco_df = telco_df.join(pd.get_dummies(telco_df['region'], prefix='region'), how='inner')\n",
        "telco_df = telco_df.drop('region', axis=1)\n",
        "telco_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIPWvPAsFXFI"
      },
      "source": [
        "## Data Transformation\n",
        "\n",
        "**We prepare the data for modeling now.**\n",
        "\n",
        "**First off we have to handle outliers of our dataset. The problem with massive outliers is that some models (like regression) can be thrown off by them.**\n",
        "\n",
        "\n",
        "**The easiest way to handle outliers is to throw the row off where we see an outlier in one of the columns. Now the problem is that our dataset is very small so we can't really afford to throw out any of them. Even if there is an outlier in one of the columns of a row, the rest of the columns contain valuable data. So what we will do instead is transform the outlier values and keep the row.**\n",
        "\n",
        "\n",
        "**In such a low row count we can't be sure if an outlier really is an outstanding value or just and undersampled portion of the real data. This is why I will be very forgiving with outliers.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:09.380003Z",
          "start_time": "2020-09-06T19:29:08.568172Z"
        },
        "id": "BqIGqcTeFXFI"
      },
      "source": [
        "big_columns = ['income', 'longten', 'tollten', 'equipten', 'cardten', 'wireten']\n",
        "small_columns = ['tenure', 'age', 'address', 'employ', 'longmon', 'tollmon', 'equipmon', 'cardmon', 'wiremon']\n",
        "\n",
        "fig = sns.boxplot(data=telco_df[big_columns], palette=\"Set2\")\n",
        "fig.set_xticklabels(fig.get_xticklabels(), rotation=-90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:10.053168Z",
          "start_time": "2020-09-06T19:29:09.382002Z"
        },
        "id": "cOH4G-pyFXFL"
      },
      "source": [
        "fig = sns.boxplot(data=telco_df[small_columns], palette=\"Set2\")\n",
        "fig.set_xticklabels(fig.get_xticklabels(), rotation=-90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T00:25:57.115988Z",
          "start_time": "2020-09-06T00:25:57.099999Z"
        },
        "id": "OlxHT4e0FXFP"
      },
      "source": [
        "**In those columns where we saw heavy outliers I will take the top 5% of the values and lower them to 95% quantile.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:10.083149Z",
          "start_time": "2020-09-06T19:29:10.055166Z"
        },
        "id": "RM8NxuosFXFP"
      },
      "source": [
        "cols_with_outliers = ['cardten', 'cardmon', 'longmon', 'longten', 'tollmon', 'tollten', 'wireten', 'wiremon']\n",
        "\n",
        "\n",
        "for col in cols_with_outliers:\n",
        "    q95 = telco_df[col].quantile(.95)\n",
        "    telco_df[col] = telco_df[col].apply(lambda val: min(val,q95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:10.636563Z",
          "start_time": "2020-09-06T19:29:10.085147Z"
        },
        "id": "KWN1Xio3FXFT"
      },
      "source": [
        "fig = sns.boxplot(data=telco_df[big_columns], palette=\"Set2\")\n",
        "fig.set_xticklabels(fig.get_xticklabels(), rotation=-90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:11.300300Z",
          "start_time": "2020-09-06T19:29:10.638561Z"
        },
        "id": "ajNsjnOqFXFW"
      },
      "source": [
        "fig = sns.boxplot(data=telco_df[small_columns], palette=\"Set2\")\n",
        "fig.set_xticklabels(fig.get_xticklabels(), rotation=-90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBqQyhPFXFY"
      },
      "source": [
        "**These boxplots do indeed seem a lot friendlier after this small adjustment to the heavy outliers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUfM7sHFXFY"
      },
      "source": [
        "## Normalization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T01:51:24.493218Z",
          "start_time": "2020-09-06T01:51:24.485223Z"
        },
        "id": "tjUdV6XMFXFZ"
      },
      "source": [
        "**A lot of the algorithms require normalization to give the best possible results. Without normalization KNN for example could give more weight to features with higher magnitude of values when calculating distances regardless of the correlation with the target variable.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:11.488192Z",
          "start_time": "2020-09-06T19:29:11.302299Z"
        },
        "id": "ISQnYKw0FXFa"
      },
      "source": [
        "telco_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:17.819880Z",
          "start_time": "2020-09-06T19:29:11.490191Z"
        },
        "id": "s7a_0Ti3FXFe"
      },
      "source": [
        "cols_to_be_normalized = ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'reside', 'longmon', 'tollmon', 'equipmon',\\\n",
        "                        'cardmon', 'wiremon', 'longten', 'tollten', 'equipten', 'cardten', 'wireten', 'custcat']\n",
        "\n",
        "\n",
        "for col in cols_to_be_normalized:\n",
        "    telco_df[col] = telco_df[col].apply(lambda x: (x - telco_df[col].min()) / \n",
        "                (telco_df[col].max() - telco_df[col].min()) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:17.992778Z",
          "start_time": "2020-09-06T19:29:17.821879Z"
        },
        "id": "hiwd8jVwFXFh"
      },
      "source": [
        "telco_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULXkMqSiFXFl"
      },
      "source": [
        "**Let's see the correlation values now that the we are done with data massaging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:19.036867Z",
          "start_time": "2020-09-06T19:29:18.010766Z"
        },
        "id": "G3rTU7QXFXFm"
      },
      "source": [
        "correlations_with_target = telco_df.corr()['churn'].values.reshape((-1,1))\n",
        "column_names = telco_df.columns\n",
        "\n",
        "sns.heatmap(correlations_with_target, annot=True, cmap='coolwarm', vmin=-1, yticklabels = column_names, xticklabels='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfS_FlCAFXFo"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:19.366664Z",
          "start_time": "2020-09-06T19:29:19.038866Z"
        },
        "id": "V9pcn0okFXFp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dYcgkeRFXFu"
      },
      "source": [
        "**For modeling I will try different models with parameter optimization. I will try 3 things to compensate for the class imbalance:**\n",
        "\n",
        "1. The scoring metric used will be F1 macro average, this will optimize for a good balance between precision and recall for both classes.\n",
        "2. Will try to rebalance the dataset using Synthetic Minority Oversampling Technique (SMOTE).\n",
        "3. Some algorithms can weight the samples to counteract class imbalances, will try this as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:19.398644Z",
          "start_time": "2020-09-06T19:29:19.368663Z"
        },
        "id": "qKGZg7O0FXFv"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:29:19.429625Z",
          "start_time": "2020-09-06T19:29:19.400643Z"
        },
        "id": "l67qhRMqFXFy"
      },
      "source": [
        "X = telco_df.drop('churn', axis=1)\n",
        "y = telco_df['churn']\n",
        "\n",
        "\n",
        "def param_optimization(model_class, params, k_range, k_selector=chi2, balancing=None, scoring='f1_macro'):\n",
        "    best_score = -1\n",
        "    best_params = None\n",
        "\n",
        "    for k in tqdm(k_range):\n",
        "        filter_kbest = SelectKBest(k_selector, k=k)\n",
        "        X_new = filter_kbest.fit_transform(telco_df.drop('churn', axis=1), telco_df['churn'])\n",
        "\n",
        "        k_best_cols = telco_df.drop('churn', axis=1).columns[filter_kbest.get_support()]\n",
        "        X_train, X_valid, y_train, y_valid \\\n",
        "        = train_test_split(telco_df[k_best_cols], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "        \n",
        "        if balancing == 'SMOTE':\n",
        "            over = SMOTE(sampling_strategy=.5)\n",
        "            under = RandomUnderSampler(sampling_strategy=0.4)\n",
        "            pipeline = Pipeline(steps=[('under', under), ('over', over)])\n",
        "            X_train, y_train = over.fit_resample(X_train, y_train)\n",
        "\n",
        "        param_optimizer = GridSearchCV(estimator = model_class(), param_grid = params, \n",
        "                                       verbose=0, n_jobs = 4, scoring=scoring)\n",
        "        param_optimizer.fit(X_train, y_train)\n",
        "        \n",
        "        \n",
        "        model = model_class(**param_optimizer.best_params_)\n",
        "        \n",
        "        if balancing == 'WEIGHTED':\n",
        "            sample_weights = [1 / (y_train.value_counts()[label] / len(y_train)) for label in y_train]            \n",
        "            model.fit(X_train, y_train, sample_weight = sample_weights)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        predictions = model.predict(X_valid)\n",
        "\n",
        "        f1_macro_score = f1_score(y_valid, predictions, average='macro')\n",
        "        if f1_macro_score > best_score:\n",
        "            best_params = {'k': k, 'k_best_cols': k_best_cols}\n",
        "            best_params['model_params'] = param_optimizer.best_params_\n",
        "            best_score = f1_macro_score\n",
        "            print(f'New Best score found! score: {best_score}, params: {best_params}')\n",
        "\n",
        "    print(f'FINISHED OPTIMIZATION! score: {best_score}, params: {best_params}')\n",
        "    return best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:30:44.038284Z",
          "start_time": "2020-09-06T19:29:19.431624Z"
        },
        "id": "-U_YfcwTFXF2"
      },
      "source": [
        "best_params = param_optimization(KNeighborsClassifier, { 'n_neighbors': range(1, 2 * len(X.columns)), 'p':[1,2]},\\\n",
        "                                 range(10,25), balancing = 'SMOTE')\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_knn = KNeighborsClassifier(**best_params['model_params'])\n",
        "model_knn.fit(X_train, y_train)\n",
        "\n",
        "predictions = model_knn.predict(X_valid)\n",
        "\n",
        "print(classification_report(y_valid,predictions))\n",
        "print(accuracy_score(y_valid, predictions))\n",
        "print(balanced_accuracy_score(y_valid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:31:52.757004Z",
          "start_time": "2020-09-06T19:30:44.040282Z"
        },
        "id": "cmuf1MehFXF5"
      },
      "source": [
        "best_params = param_optimization(KNeighborsClassifier, { 'n_neighbors': range(1, 2 * len(X.columns)), 'p':[1,2]}, range(10,25))\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_knn = KNeighborsClassifier(**best_params['model_params'])\n",
        "model_knn.fit(X_train, y_train)\n",
        "\n",
        "predictions = model_knn.predict(X_valid)\n",
        "\n",
        "print(classification_report(y_valid,predictions))\n",
        "print(accuracy_score(y_valid, predictions))\n",
        "print(balanced_accuracy_score(y_valid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T18:03:54.985275Z",
          "start_time": "2020-09-06T18:03:54.966285Z"
        },
        "id": "z-KQ0N0hFXF9"
      },
      "source": [
        "### KNN\n",
        "\n",
        "**The KNN algorithm is sensitive to outliers and needs data to be normalized. I did this during the preprocessing so the algorithm can be used.**\n",
        "\n",
        "**KNN doesn't use weighing of samples so I tried SMOTE. Unfortunately the rebalancing was not successful, maybe because of the low sample size the algorithm can't create good new samples for the churn samples, and throwing away non-churn samples is very costly regarding information.**\n",
        "\n",
        "**Best KNN model statistics> F1 macro average: 0.74, Balanced Accuracy: 0.73**\n",
        "\n",
        "**Best hyperparameters:**\n",
        "\n",
        "- Columns: \n",
        "```python\n",
        "['tenure', 'age', 'address', 'ed', 'employ', 'retire', 'equip',\n",
        "       'callcard', 'wireless', 'longmon', 'equipmon', 'cardmon', 'longten',\n",
        "       'tollten', 'cardten', 'voice', 'pager', 'internet', 'ebill',\n",
        "       'has_equipment', 'has_card', 'has_wireless']```\n",
        "- N Neighbours: 43\n",
        "- Distance metric: Manhattan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:41:16.345862Z",
          "start_time": "2020-09-06T19:31:52.760002Z"
        },
        "id": "kwWUvwrOFXF9"
      },
      "source": [
        "best_params = param_optimization(RandomForestClassifier, { 'n_estimators': range(1,50,2), 'max_depth': range(1,35,2)}\\\n",
        "                                 ,range(10,25), balancing = 'WEIGHTED')\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_rf = RandomForestClassifier(**best_params['model_params'])\n",
        "\n",
        "sample_weights = [1 / (y_train.value_counts()[label] / len(y_train)) for label in y_train]            \n",
        "model_rf.fit(X_train, y_train, sample_weight = sample_weights)\n",
        "\n",
        "predictions = model_rf.predict(X_valid)\n",
        "\n",
        "\n",
        "print(classification_report(y_valid,predictions))\n",
        "print(accuracy_score(y_valid, predictions))\n",
        "print(balanced_accuracy_score(y_valid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T19:50:25.067017Z",
          "start_time": "2020-09-06T19:41:16.348860Z"
        },
        "id": "lXt3wPBfFXGB"
      },
      "source": [
        "best_params = param_optimization(RandomForestClassifier, { 'n_estimators': range(1,50,2), 'max_depth': range(1,35,2)},\\\n",
        "                                range(10,25))\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_rf = RandomForestClassifier(**best_params['model_params'])\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "predictions = model_rf.predict(X_valid)\n",
        "\n",
        "\n",
        "print(classification_report(y_valid,predictions))\n",
        "print(accuracy_score(y_valid, predictions))\n",
        "\n",
        "\n",
        "print(balanced_accuracy_score(y_valid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXFbbzDEFXGF"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "**Tree based algorithms are very robust. They are almost ignorant to outliers and does not need data to be normalized.**\n",
        "\n",
        "**Random Forests support weighing of samples and indeed turning it on helped the learning process**\n",
        "\n",
        "**Best Random Forest model statistics> F1 macro average: 0.74, Balanced Accuracy: 0.72**\n",
        "\n",
        "**Best hyperparameters:**\n",
        "\n",
        "- Columns: \n",
        "```python\n",
        "['tenure', 'age', 'address', 'ed', 'employ', 'retire', 'equip',\n",
        "       'callcard', 'wireless', 'longmon', 'equipmon', 'cardmon', 'longten',\n",
        "       'tollten', 'cardten', 'voice', 'pager', 'internet', 'ebill',\n",
        "       'has_equipment', 'has_card', 'has_wireless']```\n",
        "- Max Depth: 7\n",
        "- N Estimators (number of trees): 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T20:08:16.949117Z",
          "start_time": "2020-09-06T20:03:59.105210Z"
        },
        "id": "Z9WssafvFXGH"
      },
      "source": [
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['linear'],\n",
        "        'C': np.linspace(.5, 6, 7),\n",
        "        'class_weight': [None, 'balanced']\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'C': np.linspace(.5, 6, 7),\n",
        "        'degree': range(5, 10),\n",
        "        'class_weight': [None, 'balanced']\n",
        "    },\n",
        "    {\n",
        "        'kernel': ['rbf'],\n",
        "        'C': np.linspace(.5, 6, 7),\n",
        "        'gamma': np.linspace(0.001, 2, 10),\n",
        "        'class_weight': [None, 'balanced']\n",
        "    }\n",
        "]\n",
        "\n",
        "best_params = param_optimization(SVC, param_grid, [10, 13, 15, 17, 22, 25])\n",
        "\n",
        "\n",
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_svc = SVC(**best_params['model_params'])\n",
        "model_svc.fit(X_train, y_train)\n",
        "\n",
        "predictions = model_svc.predict(X_valid)\n",
        "\n",
        "\n",
        "print(classification_report(y_valid,predictions))\n",
        "print(accuracy_score(y_valid, predictions))\n",
        "print(balanced_accuracy_score(y_valid, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGIxprgxFXGM"
      },
      "source": [
        "### Support Vector Machines\n",
        "\n",
        "**Support vector based models are also reliant on distance calculations thus sensitive to outliers and normalization.**\n",
        "\n",
        "**SVC also supports weighing of samples but it doesn't seem to make a significant difference. The best classifier seems to be the rbf version.**\n",
        "\n",
        "**Best SVC model statistics> F1 macro average: 0.71, Balanced Accuracy: 0.69**\n",
        "\n",
        "**Best hyperparameters:**\n",
        "\n",
        "- Columns: \n",
        "```python\n",
        "['tenure', 'age', 'address', 'ed', 'employ', 'retire', 'equip',\n",
        "       'callcard', 'wireless', 'longmon', 'equipmon', 'cardmon', 'longten',\n",
        "       'tollten', 'cardten', 'voice', 'pager', 'internet', 'ebill',\n",
        "       'has_equipment', 'has_card', 'has_wireless']```\n",
        "- kernel: rbf\n",
        "- C: 6\n",
        "- gamma: 0.22\n",
        "\n",
        "\n",
        "Note: On my machine SVC hyperparameter optimization failed multiple times and had to be restarted, in case I had access to a stronger machine I am sure this would yield the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwonZxbaFXGM"
      },
      "source": [
        "# Summary\n",
        "\n",
        "**The given model contained very few samples for reliable churn prediction, thus I had to be very gentle with outliers.**\n",
        "\n",
        "**The dataset is imbalanced and I could hardly create good synthetic data due to the low sample count.**\n",
        "\n",
        "**The dataset contained a few columns where I could extract additional information by feature engineering. The engineered *has_card*, *has_equipment* and *has_wireless* columns are among the best K features selected.**\n",
        "\n",
        "**I would advise adding 2 additional columns to the data:**\n",
        "- **competition_investment: Has there been any infrastructural investments made by the competition in the past 6 months**\n",
        "- **complaints: Has the subscriber called the service desk due to outages or any other complaints regarding the service in the past 3 months? This could even be a numeric field containing the number of calls, or the last call date.**\n",
        "\n",
        "**The best model I created was KNN with a balanced accuracy of 73%. Below we can see the confusion matrix of it on the validation set.**\n",
        "\n",
        "**In the future I would definetily try building a neural network model for the data. Also I would try a more extensive hyperparameter tuning on a stronger machine, along with a PCA dimensionality reduction.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-06T20:17:49.627059Z",
          "start_time": "2020-09-06T20:17:49.420189Z"
        },
        "id": "hhKTkfsKFXGN"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid \\\n",
        "    = train_test_split(telco_df[best_params['k_best_cols']], telco_df['churn'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model_knn.fit(X_train, y_train)\n",
        "\n",
        "predictions = model_knn.predict(X_valid)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_valid, predictions)\n",
        "\n",
        "\n",
        "sns.set(font_scale=2)\n",
        "figsize(10, 10)\n",
        "\n",
        "fig = sns.heatmap(conf_matrix, annot=True, linewidths=.5, cmap='coolwarm', cbar=False, fmt='d')\n",
        "fig.set_ylabel('Actual')\n",
        "fig.set_xlabel('Predicted')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}